
\label{sect:template_training}

In this section, we will present an approach for learning SED templates directly from broadband photometry, using a modified version of the algorithm developed in \citet{Budavari2000b}. 
If we assume that the galaxies in our data set are sampled from a small set of underlying spectra, the SED templates, and we know the spectroscopic redshift for each galaxy, we can shift the photometry to the rest frame and treat each observation of a redshifted galaxy as a rest frame observation of one of the templates with a different set of effective filters. 
With a large enough data set, the wavelengths of the effective filters will overlap substantially. 
This over-sampling allows us to recover higher resolution features in the templates, even though the data are low resolution observations of different galaxies.

Let us assume we have a set of SED templates as a starting point, which can represent rudimentary guesses and need not resemble true galaxy spectra. 
In the first part of this section, we will describe a method by which we can create a training set of broadband photometry for each template from a large data set of galaxy photometry. 
In the second part, we will derive the perturbation algorithm that is used to train each SED template on its corresponding photometry set. 
The full training algorithm is an expectation maximization that consists of iterating these two steps: matching photometry to templates, and perturbing templates to better match the photometry.
This process is iterated until the SED templates converge. 
In the final part, I will discuss a heuristic for selecting hyperparameters for the training.

\note{Work in analogy to Drizzle \citep{Fruchter2002} and to this image reconstruction paper \citep{Lee2019}.}




\subsection{Generating Training Sets}
\label{sect:training_sets}
        
Assume we have a set of naive SED templates and a large set of observed fluxes, $\{f_m\}$, with known spectroscopic redshifts, $z_m$. 
Our goal is to train each template on an appropriate subset of the $\{f_m\}$, so that the naive templates better represent the true set of SED templates from which the galaxy spectra are sampled. 
To assemble these training sets, we consider subsets $\{f_n\} \subset \{f_m\}$, corresponding to the observed fluxes of a single galaxy at redshift $z$, where the subscript $n$ denotes different filters. 
We compare these observed fluxes with the template fluxes $\{\hat{f}_n\}$, where
\begin{align}
    \hat{f}_n &= \int S\left(\frac{\lambda}{1+z}\right) R^n(\lambda) d\lambda, \label{eq:calc_flux1}
\end{align}
$S(\lambda)$ is an SED template, and $R^n(\lambda)$ is the normalized response function of the filter used to measure the flux $f_n$.
For photon counting detectors,
\begin{align}
    R(\lambda) = \frac{\lambda \, T(\lambda)}{\int \lambda \, T(\lambda) d\lambda},
\end{align}
where $T(\lambda)$ is the system response function that captures the transmittance of the atmosphere and the response of the detector.

The observed fluxes are assigned to the template whose shape is most similar, which is determined by normalizing the observed and template fluxes in the same band and picking the template that minimizes the squared differences of the fluxes. 
The normalization band is chosen by selecting the band for which the ratio $\hat{f}_n / f_n$ is the median of the flux ratios for that galaxy. 
By performing this matching and renormalization for each galaxy in the photometry set, we assemble training sets containing many galaxies for each template.

Examining how the photometry set is divided into training sets is helpful in assembling the initial set of templates. 
The initial templates should be selected so that the different spectral shapes visible in the photometry data are sorted into different training sets. 
It is also important that each training set contains a sufficient number of fluxes distributed across the wavelengths of interest, as the perturbation algorithm derived in the next section relies on over-sampling to reconstruct higher resolution features of the SED templates.




\subsection{The Perturbation Algorithm}
\label{sect:perturbation}

Assume we have a set of photometry, $\{f_n\}$, which constitute observations of the same underlying SED template, $S(\lambda)$, at various known redshifts, $z_n$. 
These observed fluxes should approximately match the template fluxes calculated via Equation \ref{eq:calc_flux1}. 
However, we can also calculate the template fluxes by imagining that we are observing the template in its rest frame using a set of effective, blueshifted filters:
\begin{align}
    \hat{f}_n = \int S(\lambda) \, R^n[(1+z_n)\lambda] \, d[(1+z_n)\lambda]. \label{eq:calc_flux2}
\end{align}

We wish to perturb the template so that the template fluxes, $\hat{f}_n$, better match the observed fluxes, $f_n$. 
Replacing $S(\lambda)$ with the discrete representation $s_k$, where k indexes wavelength bins, we can define the cost function
\begin{align}
    \chi^2 =
    \sum_n \frac{1}{\sigma_n^2}(\hat{f}_n(\{\hat{s}_k\}) - f_n)^2 + 
    \sum_k \frac{1}{\Delta_k^2}(\hat{s}_k - s_k)^2, \label{eq:cost_function}
\end{align}
with respect to the perturbed template, $\hat{s}_k$. 
The optimum perturbation to $s_k$ can then be found via a multidimensional minimization of the cost function. 
The first term in Equation \ref{eq:cost_function} penalizes differences between the observed fluxes and the perturbed template fluxes, weighted according to $\sigma_n$, the fractional error of the measured flux. 
The perturbed template fluxes can be calculated with a discretized version of Equation \ref{eq:calc_flux2}:
\begin{align}
    \hat{f}_n(\{\hat{s}_k\}) = \sum_k \hat{s}_k \, r_{k'}^n \Delta\lambda_{k'}
\end{align}
where $r_k^n$ is the discrete representation of $R^n(\lambda)$, $k'$ is the wavelength bin corresponding to $\lambda_{k'} = (1+z_n) \lambda_k$ and $\Delta\lambda_{k'} = (1+z_n)\Delta\lambda_k$, where $\Delta\lambda_k$ is the width of wavelength bin k. 
The second term in Equation \ref{eq:cost_function} penalizes large perturbations, weighted by the hyperparameters $\Delta_k$. 
This parameter controls learning rate and also helps stabilize the results. 
See the next section for more details. 

We follow \citet{Budavari2000b} by introducing the simplifying perturbation and constant terms: 
\begin{align}
    \begin{gathered}
        \xi_k = \hat{s}_k - s_k \\
        g_n = f_n - \sum_k s_k \, r_{k'}^n \Delta\lambda_{k'}.
    \end{gathered}
\end{align} 
Then, we have:
\begin{align}
    \chi^2 = 
    \sum_n \frac{1}{\sigma_n^2} \left( g_n - \sum_k \xi_k \,  r_{k'}^n \Delta\lambda_{k'} \right)^2 +
    \sum_k \frac{\xi_k^2}{\Delta_k^2},
\end{align}
which can be analytically minimized:
\begin{align}
    \frac{\partial \chi^2}{\partial \xi_l} = 0 \implies \sum_k M_{lk} \xi_k = \nu_l.
\end{align}
The matrix $M$ and vector $\nu$ are defined
\begin{align}
    \begin{gathered}
        M_{lk} = \sum_n \frac{1}{\sigma_n^2} (r_{l'}^n \Delta\lambda_{l'}) (r_{k'}^n \Delta\lambda_{k'}) + \frac{\delta_{lk}}{\Delta_k^2}, \\
        \nu_l = \sum_n \frac{g_n}{\sigma_n^2} (r_{l'}^n \Delta\lambda_{l'}),
    \end{gathered}
\end{align}
where $\delta_{lk}$ is the Kronecker delta.
One can numerically solve for $\xi$. 
The perturbed spectrum is then $\hat{s}_k = s_k + \xi_k$. 

\begin{figure}
    \centering
    \includegraphics{figures/training_example.pdf}
    \caption{Perturbing a naive template, in this case a flat line, to better match a photometry set. Top: the orange points are simulated observations of the 5Myr starburst template from \citet{Coe2006a} at 1,000 random redshifts in the range z=0 to z=3 using the $ugrizY$ filters listed in Table \ref{tab:filters}. The simulated photometry has 10\% Gaussian error. The template is shown after various stages of the training. Bottom: the learned template is plotted with the original starburst template.}
    \label{fig:pert_ex}
\end{figure}

Iterating the perturbations changes the shape of the template SED to better match the measured photometry. 
An example of this can be seen in Figure \ref{fig:pert_ex}. 
Fluxes in the $ugrizY$ filters listed in Table \ref{tab:filters} were calculated for a starburst galaxy template at 1,000 random redshifts in the range z=0 to z=3. 
Starting with an $S(\lambda) = 0$ template SED, the perturbation algorithm is applied iteratively. 
After 100 iterations, the learned template closely matches the original template in the wavelength range for which photometry exists. 
While the learned template is a smoothed version of the original, high resolution features have been recovered, despite the relatively low resolution of the filters. 
Note that in practice, higher $\Delta_k$ can be chosen so that fewer iterations are required in the training. 
A lower value was chosen here so that the effects of successive iterations can be more clearly seen.
See Section \ref{sect:hyperparameters} for further discussion of selecting the hyperparameters.

The perturbation algorithm changes the shapes of the template SED's, so that re-running the algorithm to generate training sets will now result in different sets than previously obtained. 
As better approximations of the true underlying SED templates are generated by the perturbation algorithm, the training set generation algorithm will do a better and better job at dividing the photometry data into training sets that characterize distinct spectral shapes. 
We iterate the full training algorithm (generate training sets, perturb the templates, re-generate training sets, perturb the templates again, etc), until the SED templates converge.




\subsection{Selecting Hyperparameters}
\label{sect:hyperparameters}

The training algorithm relies on the choice of four hyperparameters. 
The first is the number of templates. 
As discussed in Section \ref{sect:training_sets}, this choice can be made by using the training set generation algorithm and choosing the appropriate number of templates to roughly separate out the different spectral shapes displayed in the photometry.
For further discussion of how the number of templates effects photo-z results, see Section \note{REF TEMPLATE \# SECTION}.

The final three hyperparameters control the template perturbation algorithm, namely the values of the $\Delta_k$, the number of iterations ($N_\text{iter}$), and the number of template perturbations per iteration ($N_\text{pert}$). 
These three are interconnected and choosing them appropriately affects the stability and speed of the training algorithm. 
Here we present a heuristic for selecting the $\Delta_k$.
Discussion of $N_iter$ and $N_pert$ is deferred to Section \ref{sect:template_training}.

The most important parameter to select is $\Delta_k$, which controls the relative weighting of the regularization term in Equation \ref{eq:cost_function}. 
If this value is too large, training will be very slow and a large number of perturbation and training rounds will be required. 
If this value is too low, the training becomes unstable and the final templates will be over-fit.

For the work presented below, the index $k$ is dropped, so that $\Delta \equiv \Delta_k$ has a single value for each training set that is independent of wavelength. 
In choosing the appropriate value of $\Delta$ for each training set, it is desirable to select a value that corresponds to a constant ratio, $w$, of the flux and regularization terms in Equation \ref{eq:cost_function}. 
The necessary value of $\Delta$ will vary by training set, as the number of terms in the sum over fluxes (i.e. the sum over $n$ in Equation \ref{eq:cost_function}) will vary by training set. 
To this end, we make the following approximation:
\begin{align}
    \frac{\sum_k \left(\hat{s}_k - s_k \right)^2}{\sum_n \left(\hat{f}_n - f_n \right)^2} \sim \frac{N_k}{N_n},
\end{align}
where $N_k \equiv \sum_k$ and $N_n \equiv \sum_n$. 
This permits the following approximation of the ratio $w$: 
\begin{align}
    w = \frac{\sum_k \frac{1}{\Delta^2} \left(\hat{s}_k - s_k \right)^2}{\sum_n \frac{1}{\sigma_n^2} \left(\hat{f}_n - f_n \right)^2} \sim \frac{N_k/\Delta^2}{N_n/\Bar{\sigma}^2},
\end{align}
where $\Bar{\sigma} = \sum_n \sigma_n/N_n$. 
Then, for a desired ratio $w$, the requisite $\Delta$ can be approximated:
\begin{align}
    \Delta \simeq \Bar{\sigma} \sqrt{\frac{N_k}{w N_n}}.
\end{align}
In practice, we have found that $w = \mathcal{O}(1)$ works well.
The results of the training are relatively robust to the selection of $w$, in that changing $w$ by, for example, a factor of 2 yields roughly the same results.
